{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "action_object_location_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4jrT97KEc6u"
      },
      "source": [
        "###Checkpoints\n",
        "* Modelling - Done\n",
        "* config file and .py files for test and train - Done\n",
        "* Multi GPU training - Done\n",
        "* CPU inferencing which outputs F1 score - Done\n",
        "* Logging of training - Done\n",
        "* Documentation of project - PPT Done, Report Done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKDm7tCIUCr5"
      },
      "source": [
        "# RUNNING CODE FROM .PY AND CONFIG FILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbQ05vs8Q4Bd"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsauMBq9RI6H",
        "outputId": "3a1ae9f6-e89c-42dd-d35b-3f4d0c7ffceb"
      },
      "source": [
        "!python3 saarthi_train.py --config external.json"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.104.183.170:8470\n",
            "2021-11-16 04:30:28.809592: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Using 8 TPUs\n",
            "Validation data consists of same sentences as train data if the sound path is ignored.\n",
            "New train shape- (200, 4) , New val shape- (48, 4)\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 161s 11s/step - loss: 5.8038 - action_loss: 1.7955 - object_loss: 2.6987 - location_loss: 1.3096 - action_sparse_categorical_accuracy: 0.2050 - action_Top_3: 0.4750 - object_sparse_categorical_accuracy: 0.0100 - object_Top_3: 0.0950 - location_sparse_categorical_accuracy: 0.2900 - location_Top_3: 0.8900 - val_loss: 5.6392 - val_action_loss: 1.7903 - val_object_loss: 2.5863 - val_location_loss: 1.2625 - val_action_sparse_categorical_accuracy: 0.2708 - val_action_Top_3: 0.4792 - val_object_sparse_categorical_accuracy: 0.0208 - val_object_Top_3: 0.3542 - val_location_sparse_categorical_accuracy: 0.5625 - val_location_Top_3: 0.8750\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 2s 323ms/step - loss: 5.4772 - action_loss: 1.7438 - object_loss: 2.5432 - location_loss: 1.1902 - action_sparse_categorical_accuracy: 0.2500 - action_Top_3: 0.6200 - object_sparse_categorical_accuracy: 0.1600 - object_Top_3: 0.3400 - location_sparse_categorical_accuracy: 0.6200 - location_Top_3: 0.8900 - val_loss: 5.2593 - val_action_loss: 1.7704 - val_object_loss: 2.3401 - val_location_loss: 1.1488 - val_action_sparse_categorical_accuracy: 0.2708 - val_action_Top_3: 0.5208 - val_object_sparse_categorical_accuracy: 0.2917 - val_object_Top_3: 0.6250 - val_location_sparse_categorical_accuracy: 0.5625 - val_location_Top_3: 0.8750\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 2s 327ms/step - loss: 4.9284 - action_loss: 1.6639 - object_loss: 2.1753 - location_loss: 1.0892 - action_sparse_categorical_accuracy: 0.2950 - action_Top_3: 0.7150 - object_sparse_categorical_accuracy: 0.3200 - object_Top_3: 0.6950 - location_sparse_categorical_accuracy: 0.6200 - location_Top_3: 0.9100 - val_loss: 4.6559 - val_action_loss: 1.7324 - val_object_loss: 1.9020 - val_location_loss: 1.0215 - val_action_sparse_categorical_accuracy: 0.1250 - val_action_Top_3: 0.5208 - val_object_sparse_categorical_accuracy: 0.2917 - val_object_Top_3: 0.6875 - val_location_sparse_categorical_accuracy: 0.5625 - val_location_Top_3: 0.8542\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 3s 414ms/step - loss: 4.1476 - action_loss: 1.4848 - object_loss: 1.7865 - location_loss: 0.8763 - action_sparse_categorical_accuracy: 0.3950 - action_Top_3: 0.7400 - object_sparse_categorical_accuracy: 0.4200 - object_Top_3: 0.7100 - location_sparse_categorical_accuracy: 0.6250 - location_Top_3: 0.9050 - val_loss: 3.7039 - val_action_loss: 1.4146 - val_object_loss: 1.5297 - val_location_loss: 0.7596 - val_action_sparse_categorical_accuracy: 0.3333 - val_action_Top_3: 0.8542 - val_object_sparse_categorical_accuracy: 0.6458 - val_object_Top_3: 0.6875 - val_location_sparse_categorical_accuracy: 0.7292 - val_location_Top_3: 0.8958\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 2s 330ms/step - loss: 3.3274 - action_loss: 1.2477 - object_loss: 1.3870 - location_loss: 0.6927 - action_sparse_categorical_accuracy: 0.5350 - action_Top_3: 0.9000 - object_sparse_categorical_accuracy: 0.6750 - object_Top_3: 0.7400 - location_sparse_categorical_accuracy: 0.7600 - location_Top_3: 0.9350 - val_loss: 2.8927 - val_action_loss: 1.1555 - val_object_loss: 1.1553 - val_location_loss: 0.5819 - val_action_sparse_categorical_accuracy: 0.4792 - val_action_Top_3: 0.9583 - val_object_sparse_categorical_accuracy: 0.6667 - val_object_Top_3: 0.8125 - val_location_sparse_categorical_accuracy: 0.9167 - val_location_Top_3: 1.0000\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 2s 335ms/step - loss: 2.6233 - action_loss: 1.0021 - object_loss: 1.0793 - location_loss: 0.5420 - action_sparse_categorical_accuracy: 0.6950 - action_Top_3: 0.9600 - object_sparse_categorical_accuracy: 0.7150 - object_Top_3: 0.8450 - location_sparse_categorical_accuracy: 0.8700 - location_Top_3: 0.9700 - val_loss: 2.3600 - val_action_loss: 0.9674 - val_object_loss: 0.9130 - val_location_loss: 0.4796 - val_action_sparse_categorical_accuracy: 0.5208 - val_action_Top_3: 0.9583 - val_object_sparse_categorical_accuracy: 0.6875 - val_object_Top_3: 0.9167 - val_location_sparse_categorical_accuracy: 0.9375 - val_location_Top_3: 1.0000\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 2s 342ms/step - loss: 2.1008 - action_loss: 0.8415 - object_loss: 0.8522 - location_loss: 0.4072 - action_sparse_categorical_accuracy: 0.6850 - action_Top_3: 0.9650 - object_sparse_categorical_accuracy: 0.7300 - object_Top_3: 0.9150 - location_sparse_categorical_accuracy: 0.9500 - location_Top_3: 1.0000 - val_loss: 1.9085 - val_action_loss: 0.7960 - val_object_loss: 0.7706 - val_location_loss: 0.3418 - val_action_sparse_categorical_accuracy: 0.5625 - val_action_Top_3: 0.9792 - val_object_sparse_categorical_accuracy: 0.7500 - val_object_Top_3: 0.9375 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 2s 341ms/step - loss: 1.7560 - action_loss: 0.7112 - object_loss: 0.7214 - location_loss: 0.3234 - action_sparse_categorical_accuracy: 0.7700 - action_Top_3: 0.9700 - object_sparse_categorical_accuracy: 0.8150 - object_Top_3: 0.9600 - location_sparse_categorical_accuracy: 0.9600 - location_Top_3: 1.0000 - val_loss: 1.5707 - val_action_loss: 0.6918 - val_object_loss: 0.6566 - val_location_loss: 0.2223 - val_action_sparse_categorical_accuracy: 0.6250 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.8333 - val_object_Top_3: 0.9375 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 2s 332ms/step - loss: 1.3967 - action_loss: 0.5878 - object_loss: 0.5805 - location_loss: 0.2283 - action_sparse_categorical_accuracy: 0.8250 - action_Top_3: 0.9950 - object_sparse_categorical_accuracy: 0.9150 - object_Top_3: 0.9750 - location_sparse_categorical_accuracy: 0.9900 - location_Top_3: 1.0000 - val_loss: 1.3721 - val_action_loss: 0.6264 - val_object_loss: 0.5877 - val_location_loss: 0.1581 - val_action_sparse_categorical_accuracy: 0.6667 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.8333 - val_object_Top_3: 0.9375 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 2s 328ms/step - loss: 1.1582 - action_loss: 0.4900 - object_loss: 0.4992 - location_loss: 0.1690 - action_sparse_categorical_accuracy: 0.8700 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9200 - object_Top_3: 0.9700 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 1.1150 - val_action_loss: 0.4962 - val_object_loss: 0.5196 - val_location_loss: 0.0991 - val_action_sparse_categorical_accuracy: 0.8542 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.8542 - val_object_Top_3: 0.9375 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 2s 331ms/step - loss: 0.9093 - action_loss: 0.3827 - object_loss: 0.3955 - location_loss: 0.1310 - action_sparse_categorical_accuracy: 0.9450 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9500 - object_Top_3: 0.9800 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.8966 - val_action_loss: 0.3647 - val_object_loss: 0.4396 - val_location_loss: 0.0922 - val_action_sparse_categorical_accuracy: 0.9167 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.8542 - val_object_Top_3: 0.9375 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 2s 337ms/step - loss: 0.7278 - action_loss: 0.2668 - object_loss: 0.3408 - location_loss: 0.1202 - action_sparse_categorical_accuracy: 0.9850 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9650 - object_Top_3: 0.9800 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.7320 - val_action_loss: 0.2791 - val_object_loss: 0.3946 - val_location_loss: 0.0583 - val_action_sparse_categorical_accuracy: 0.9375 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.8750 - val_object_Top_3: 0.9375 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 2s 343ms/step - loss: 0.6026 - action_loss: 0.2083 - object_loss: 0.3037 - location_loss: 0.0906 - action_sparse_categorical_accuracy: 0.9950 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9550 - object_Top_3: 0.9850 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.6097 - val_action_loss: 0.2110 - val_object_loss: 0.3492 - val_location_loss: 0.0496 - val_action_sparse_categorical_accuracy: 0.9375 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9167 - val_object_Top_3: 0.9375 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 2s 344ms/step - loss: 0.4891 - action_loss: 0.1582 - object_loss: 0.2531 - location_loss: 0.0779 - action_sparse_categorical_accuracy: 0.9850 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9800 - object_Top_3: 0.9850 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.5089 - val_action_loss: 0.1640 - val_object_loss: 0.3014 - val_location_loss: 0.0435 - val_action_sparse_categorical_accuracy: 0.9375 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9167 - val_object_Top_3: 0.9375 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 2s 345ms/step - loss: 0.3962 - action_loss: 0.1135 - object_loss: 0.2207 - location_loss: 0.0620 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9700 - object_Top_3: 0.9950 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.4218 - val_action_loss: 0.1268 - val_object_loss: 0.2600 - val_location_loss: 0.0349 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9375 - val_object_Top_3: 0.9792 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 3s 473ms/step - loss: 0.3429 - action_loss: 0.1012 - object_loss: 0.1851 - location_loss: 0.0566 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9800 - object_Top_3: 0.9950 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.3611 - val_action_loss: 0.1151 - val_object_loss: 0.2173 - val_location_loss: 0.0287 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9375 - val_object_Top_3: 0.9792 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 2s 336ms/step - loss: 0.2775 - action_loss: 0.0750 - object_loss: 0.1600 - location_loss: 0.0425 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.3010 - val_action_loss: 0.0900 - val_object_loss: 0.1850 - val_location_loss: 0.0260 - val_action_sparse_categorical_accuracy: 1.0000 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9375 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 2s 333ms/step - loss: 0.2347 - action_loss: 0.0628 - object_loss: 0.1302 - location_loss: 0.0418 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9950 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.2544 - val_action_loss: 0.0831 - val_object_loss: 0.1506 - val_location_loss: 0.0207 - val_action_sparse_categorical_accuracy: 0.9792 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 2s 339ms/step - loss: 0.1988 - action_loss: 0.0552 - object_loss: 0.1084 - location_loss: 0.0352 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.2119 - val_action_loss: 0.0715 - val_object_loss: 0.1246 - val_location_loss: 0.0158 - val_action_sparse_categorical_accuracy: 1.0000 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 2s 335ms/step - loss: 0.1741 - action_loss: 0.0505 - object_loss: 0.0952 - location_loss: 0.0284 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1871 - val_action_loss: 0.0681 - val_object_loss: 0.1035 - val_location_loss: 0.0155 - val_action_sparse_categorical_accuracy: 1.0000 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 1.0000 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciOEu_80SvAY",
        "outputId": "de842ac3-9309-44e1-f86b-3837bd9aa946"
      },
      "source": [
        "!python3 saarthi_test.py --config 'external.json'  "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.104.183.170:8470\n",
            "2021-11-16 04:35:36.465814: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Using 8 TPUs\n",
            "Test shape is (248, 4)\n",
            "F1 score for action--> 1.0\n",
            "F1 score for object--> 1.0\n",
            "F1 score for location--> 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl3C-HAeUNnS"
      },
      "source": [
        "# RUNNING IN COLAB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiqGmYeCNdgf"
      },
      "source": [
        "!pip install transformers --quiet\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import TFRobertaModel\n",
        "from keras.callbacks import CSVLogger\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "import logging, sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "logging.disable(sys.maxsize)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLvMkEFFqEOb",
        "outputId": "222730e1-e2b6-4ef6-d331-1f286c0089ad"
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy, if tpus are available tpus are used\n",
        "# Else if gpus are available gpus are used. If neither are available computation is done with CPUs\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    # Distribution strategy if tpus are available and is to be used\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print('Using {} TPUs'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "elif tf.config.list_physical_devices('GPU'):\n",
        "    # Distribution strategy in case of multiple GPUs\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print('Using {} GPUs'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print('No GPU nor TPU. Running on CPU')\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.104.183.170:8470\n",
            "Using 8 TPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKMxUe90AF_I"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 13\n",
        "EPOCHS = 20\n",
        "NUM_ACTION = 6\n",
        "NUM_OBJECT = 14\n",
        "NUM_LOCATION = 4\n",
        "FOLDER_PATH = '/content/drive/MyDrive/Saarthi/'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hovL3VEl-6M7"
      },
      "source": [
        "# TRAINING (IGNORE IF NOT NEEDED)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdtpJP9CzNgh",
        "outputId": "261b7b06-2e5a-4132-b828-5145d9d3e436"
      },
      "source": [
        "train_df = pd.read_csv(FOLDER_PATH+'train_data.csv')\n",
        "val_df = pd.read_csv(FOLDER_PATH+'valid_data.csv')\n",
        "set1 = set(train_df['transcription'])\n",
        "set2 = set(val_df['transcription'])\n",
        "print('Validation data consists of same sentences as train data if the sound path is ignored.')\n",
        "train_df.drop('path',axis='columns', inplace=True)\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "full = train_df.sample(frac=1).reset_index(drop=True)\n",
        "train_df = full.loc[:199,:]\n",
        "val_df = full.loc[200:,:]\n",
        "print(\"New train shape-\",train_df.shape,\", New val shape-\",val_df.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data consists of same sentences as train data if the sound path is ignored.\n",
            "New train shape- (200, 4) , New val shape- (48, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7DSTAF0O3md"
      },
      "source": [
        "action_enc = LabelEncoder()\n",
        "action_train = action_enc.fit_transform(train_df['action'])\n",
        "action_val = action_enc.transform(val_df['action'])\n",
        "\n",
        "object_enc = LabelEncoder()\n",
        "object_train = object_enc.fit_transform(train_df['object'])\n",
        "object_val = object_enc.transform(val_df['object'])\n",
        "\n",
        "location_enc = LabelEncoder()\n",
        "location_train = location_enc.fit_transform(train_df['location'])\n",
        "location_val = location_enc.transform(val_df['location'])\n",
        "\n",
        "texts = train_df['transcription'].values\n",
        "texts = list(texts)\n",
        "val_texts = val_df['transcription'].values\n",
        "val_texts = list(val_texts)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "train_data = tokenizer(texts, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='tf')\n",
        "val_data = tokenizer(val_texts, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='tf')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y_UJfG5PEm1"
      },
      "source": [
        "y_train = {'action': action_train, 'object':object_train, 'location': location_train}\n",
        "y_val = {'action': action_val, 'object':object_val, 'location': location_val}\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_data), y_train)).batch(BATCH_SIZE)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_data), y_val)).batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swfDznpzPEkV",
        "outputId": "18f0b622-ff82-4919-f0c6-d13358eefd7d"
      },
      "source": [
        "%%time\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
        "    bert_model = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
        "    x = bert_model(ids,attention_mask=att)\n",
        "    x1 = tf.keras.layers.Flatten()(x[1])\n",
        "    x1 = tf.keras.layers.Dense(NUM_ACTION, name='action')(x1)\n",
        "\n",
        "    x2 = tf.keras.layers.Flatten()(x[1])\n",
        "    x2 = tf.keras.layers.Dense(NUM_OBJECT, name='object')(x2)\n",
        "\n",
        "    x3 = tf.keras.layers.Flatten()(x[1])\n",
        "    x3 = tf.keras.layers.Dense(NUM_LOCATION, name='location')(x3)\n",
        "    model = tf.keras.models.Model(inputs=[ids, att], outputs=[x1,x2,x3])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='Top_3')],\n",
        "        )\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "    model = build_model()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13 s, sys: 6.37 s, total: 19.4 s\n",
            "Wall time: 43.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew9-sHFmhWAp",
        "outputId": "ff7aebbf-3520-4165-dd73-23c0ccadb56e"
      },
      "source": [
        "csv_logger = CSVLogger(FOLDER_PATH+'log.csv', append=True, separator=';')\n",
        "history=model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, verbose=1, callbacks=[csv_logger])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 2s 364ms/step - loss: 0.9599 - action_loss: 0.3188 - object_loss: 0.4977 - location_loss: 0.1434 - action_sparse_categorical_accuracy: 0.9750 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9000 - object_Top_3: 0.9850 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.5830 - val_action_loss: 0.2422 - val_object_loss: 0.2784 - val_location_loss: 0.0624 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9375 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 2s 360ms/step - loss: 0.7444 - action_loss: 0.2406 - object_loss: 0.3970 - location_loss: 0.1069 - action_sparse_categorical_accuracy: 0.9800 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9500 - object_Top_3: 0.9900 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.4565 - val_action_loss: 0.1749 - val_object_loss: 0.2282 - val_location_loss: 0.0534 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9583 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 5s 893ms/step - loss: 0.5968 - action_loss: 0.1830 - object_loss: 0.3210 - location_loss: 0.0928 - action_sparse_categorical_accuracy: 0.9850 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9600 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.3683 - val_action_loss: 0.1359 - val_object_loss: 0.1912 - val_location_loss: 0.0411 - val_action_sparse_categorical_accuracy: 0.9375 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9583 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 2s 359ms/step - loss: 0.4937 - action_loss: 0.1430 - object_loss: 0.2764 - location_loss: 0.0742 - action_sparse_categorical_accuracy: 0.9900 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9600 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.2952 - val_action_loss: 0.1223 - val_object_loss: 0.1457 - val_location_loss: 0.0272 - val_action_sparse_categorical_accuracy: 0.9792 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 2s 359ms/step - loss: 0.3946 - action_loss: 0.1104 - object_loss: 0.2313 - location_loss: 0.0529 - action_sparse_categorical_accuracy: 0.9950 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9800 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.2580 - val_action_loss: 0.1160 - val_object_loss: 0.1177 - val_location_loss: 0.0244 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 2s 352ms/step - loss: 0.3122 - action_loss: 0.0871 - object_loss: 0.1769 - location_loss: 0.0482 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9950 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1952 - val_action_loss: 0.0870 - val_object_loss: 0.0881 - val_location_loss: 0.0201 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 2s 367ms/step - loss: 0.2634 - action_loss: 0.0730 - object_loss: 0.1480 - location_loss: 0.0423 - action_sparse_categorical_accuracy: 0.9950 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9900 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1737 - val_action_loss: 0.0756 - val_object_loss: 0.0776 - val_location_loss: 0.0206 - val_action_sparse_categorical_accuracy: 0.9792 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 2s 358ms/step - loss: 0.2186 - action_loss: 0.0589 - object_loss: 0.1221 - location_loss: 0.0376 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1528 - val_action_loss: 0.0722 - val_object_loss: 0.0640 - val_location_loss: 0.0167 - val_action_sparse_categorical_accuracy: 0.9792 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 2s 355ms/step - loss: 0.1767 - action_loss: 0.0516 - object_loss: 0.0925 - location_loss: 0.0326 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1137 - val_action_loss: 0.0516 - val_object_loss: 0.0484 - val_location_loss: 0.0137 - val_action_sparse_categorical_accuracy: 1.0000 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 1.0000 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 2s 352ms/step - loss: 0.1557 - action_loss: 0.0440 - object_loss: 0.0845 - location_loss: 0.0272 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.0962 - val_action_loss: 0.0475 - val_object_loss: 0.0372 - val_location_loss: 0.0115 - val_action_sparse_categorical_accuracy: 1.0000 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 1.0000 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M0qkxdr8hmN"
      },
      "source": [
        "model.save_weights(FOLDER_PATH+'model.h5')\n",
        "np.save(FOLDER_PATH+'action_encoder.npy', action_enc.classes_)\n",
        "np.save(FOLDER_PATH+'object_encoder.npy', object_enc.classes_)\n",
        "np.save(FOLDER_PATH+'location_encoder.npy', location_enc.classes_)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpTBK1Mu66H"
      },
      "source": [
        "# PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AIpdhk9ahwA",
        "outputId": "d1c7dc5b-1149-4854-cf27-a8207269fd81"
      },
      "source": [
        "#folder containing model and the encoders\n",
        "FOLDER_PATH = '/content/drive/MyDrive/Saarthi/'\n",
        "#path to test dataset\n",
        "TEST_PATH = FOLDER_PATH+'valid_data.csv'\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "test_df.drop('path',axis='columns', inplace=True)\n",
        "test_df.drop_duplicates(inplace=True)\n",
        "print(\"Test shape is\", test_df.shape)\n",
        "\n",
        "action_enc = LabelEncoder()\n",
        "action_enc.classes_ = np.load(FOLDER_PATH+'action_encoder.npy', allow_pickle=True)\n",
        "action_test = action_enc.transform(test_df['action'])\n",
        "\n",
        "object_enc = LabelEncoder()\n",
        "object_enc.classes_ = np.load(FOLDER_PATH+'object_encoder.npy', allow_pickle=True)\n",
        "object_test = object_enc.transform(test_df['object'])\n",
        "\n",
        "location_enc = LabelEncoder()\n",
        "location_enc.classes_ = np.load(FOLDER_PATH+'location_encoder.npy', allow_pickle=True)\n",
        "location_test = location_enc.transform(test_df['location'])\n",
        "\n",
        "test_texts = test_df['transcription'].values\n",
        "test_texts = list(test_texts)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "MAX_LEN = 13\n",
        "test_data = tokenizer(test_texts, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='tf')\n",
        "y_test = {'action': action_test, 'object':object_test, 'location': location_test}\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_data), y_test)).batch(BATCH_SIZE)\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
        "    bert_model = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
        "    x = bert_model(ids,attention_mask=att)\n",
        "    x1 = tf.keras.layers.Flatten()(x[1])\n",
        "    x1 = tf.keras.layers.Dense(NUM_ACTION, name='action')(x1)\n",
        "\n",
        "    x2 = tf.keras.layers.Flatten()(x[1])\n",
        "    x2 = tf.keras.layers.Dense(NUM_OBJECT, name='object')(x2)\n",
        "\n",
        "    x3 = tf.keras.layers.Flatten()(x[1])\n",
        "    x3 = tf.keras.layers.Dense(NUM_LOCATION, name='location')(x3)\n",
        "    model = tf.keras.models.Model(inputs=[ids, att], outputs=[x1,x2,x3])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='Top_3')],\n",
        "        )\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "    model = build_model()\n",
        "\n",
        "model.load_weights(FOLDER_PATH+'model.h5')\n",
        "ans = model.predict(test_dataset)\n",
        "\n",
        "action_preds = action_enc.inverse_transform(np.argmax(ans[0],axis=1))\n",
        "object_preds = object_enc.inverse_transform(np.argmax(ans[1],axis=1))\n",
        "location_preds = location_enc.inverse_transform(np.argmax(ans[2],axis=1))\n",
        "\n",
        "pred_df = pd.DataFrame({'input':test_df['transcription'],'action':test_df['action'],'object':test_df['object'],'location':test_df['location'],\n",
        "                        'action_preds':action_preds, 'object_preds':object_preds, 'location_preds':location_preds})\n",
        "\n",
        "# micro f1 score\n",
        "action_f1 = f1_score(pred_df['action_preds'],pred_df['action'], average='micro')\n",
        "object_f1 = f1_score(pred_df['object_preds'],pred_df['object'], average='micro')\n",
        "location_f1 = f1_score(pred_df['location_preds'],pred_df['location'], average='micro')\n",
        "print('F1 score for action-->',action_f1)\n",
        "print('F1 score for object-->',object_f1)\n",
        "print('F1 score for location-->',location_f1)\n",
        "pred_df.to_csv(FOLDER_PATH+'predictions.csv',index=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test shape is (248, 4)\n",
            "F1 score for action--> 1.0\n",
            "F1 score for object--> 1.0\n",
            "F1 score for location--> 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvC88KRREKro",
        "outputId": "dbb0ddd8-b8da-4fca-a4fa-42944abf70ce"
      },
      "source": [
        "print(pred_df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          input  ... location_preds\n",
            "0                            Turn on the lights  ...           none\n",
            "1                           Turn off the lights  ...           none\n",
            "2                               Change language  ...           none\n",
            "3                               Pause the music  ...           none\n",
            "4                                        Resume  ...           none\n",
            "..                                          ...  ...            ...\n",
            "243                Turn the washroom lights off  ...       washroom\n",
            "244                                       Pause  ...           none\n",
            "326                  Lights off in the washroom  ...       washroom\n",
            "418                           Bedroom heat down  ...        bedroom\n",
            "478  OK now switch the main language to Chinese  ...           none\n",
            "\n",
            "[248 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOZLD1pwYGfd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}