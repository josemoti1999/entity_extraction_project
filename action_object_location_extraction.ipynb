{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "action_object_location_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4jrT97KEc6u"
      },
      "source": [
        "###Checkpoints\n",
        "* Modelling - Done\n",
        "* config file and .py files for test and train - Done\n",
        "* Multi GPU training - Done\n",
        "* CPU inferencing which outputs F1 score - Done\n",
        "* Logging of training - Done\n",
        "* Documentation of project - PPT Done, Report Done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKDm7tCIUCr5"
      },
      "source": [
        "# RUNNING CODE FROM .PY AND CONFIG FILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbQ05vs8Q4Bd"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsauMBq9RI6H",
        "outputId": "1f2a6220-e38a-4dd6-a4c4-baf8a2c5c2a1"
      },
      "source": [
        "!python3 saarthi_train.py --config external.json"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.104.183.170:8470\n",
            "2021-11-16 04:13:43.960829: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Using 8 TPUs\n",
            "Validation data consists of same sentences as train data if the sound path is ignored.\n",
            "New train shape- (200, 4) , New val shape- (48, 4)\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 109s 109s/step - loss: 5.7172 - action_loss: 1.7929 - object_loss: 2.5597 - location_loss: 1.3647 - action_sparse_categorical_accuracy: 0.2450 - action_Top_3: 0.5400 - object_sparse_categorical_accuracy: 0.1700 - object_Top_3: 0.2550 - location_sparse_categorical_accuracy: 0.1200 - location_Top_3: 0.8350 - val_loss: 5.6097 - val_action_loss: 1.8038 - val_object_loss: 2.4835 - val_location_loss: 1.3224 - val_action_sparse_categorical_accuracy: 0.3750 - val_action_Top_3: 0.5208 - val_object_sparse_categorical_accuracy: 0.2500 - val_object_Top_3: 0.3125 - val_location_sparse_categorical_accuracy: 0.1458 - val_location_Top_3: 0.8750\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.6476 - action_loss: 1.7703 - object_loss: 2.5451 - location_loss: 1.3322 - action_sparse_categorical_accuracy: 0.2650 - action_Top_3: 0.5650 - object_sparse_categorical_accuracy: 0.1650 - object_Top_3: 0.2600 - location_sparse_categorical_accuracy: 0.2100 - location_Top_3: 0.8500 - val_loss: 5.5677 - val_action_loss: 1.8014 - val_object_loss: 2.4645 - val_location_loss: 1.3019 - val_action_sparse_categorical_accuracy: 0.3333 - val_action_Top_3: 0.5208 - val_object_sparse_categorical_accuracy: 0.2500 - val_object_Top_3: 0.4375 - val_location_sparse_categorical_accuracy: 0.5833 - val_location_Top_3: 0.8750\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.6189 - action_loss: 1.7744 - object_loss: 2.5288 - location_loss: 1.3158 - action_sparse_categorical_accuracy: 0.2350 - action_Top_3: 0.5900 - object_sparse_categorical_accuracy: 0.1650 - object_Top_3: 0.3300 - location_sparse_categorical_accuracy: 0.5500 - location_Top_3: 0.8300 - val_loss: 5.5368 - val_action_loss: 1.8004 - val_object_loss: 2.4481 - val_location_loss: 1.2882 - val_action_sparse_categorical_accuracy: 0.2708 - val_action_Top_3: 0.5417 - val_object_sparse_categorical_accuracy: 0.2500 - val_object_Top_3: 0.5625 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.8750\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.6063 - action_loss: 1.7959 - object_loss: 2.5155 - location_loss: 1.2950 - action_sparse_categorical_accuracy: 0.2150 - action_Top_3: 0.5550 - object_sparse_categorical_accuracy: 0.1850 - object_Top_3: 0.4500 - location_sparse_categorical_accuracy: 0.5950 - location_Top_3: 0.8400 - val_loss: 5.4397 - val_action_loss: 1.7987 - val_object_loss: 2.3953 - val_location_loss: 1.2458 - val_action_sparse_categorical_accuracy: 0.1458 - val_action_Top_3: 0.4792 - val_object_sparse_categorical_accuracy: 0.2500 - val_object_Top_3: 0.6042 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.8750\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.4765 - action_loss: 1.7763 - object_loss: 2.4479 - location_loss: 1.2524 - action_sparse_categorical_accuracy: 0.2250 - action_Top_3: 0.6000 - object_sparse_categorical_accuracy: 0.1850 - object_Top_3: 0.5550 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8150 - val_loss: 5.3846 - val_action_loss: 1.7974 - val_object_loss: 2.3641 - val_location_loss: 1.2231 - val_action_sparse_categorical_accuracy: 0.1875 - val_action_Top_3: 0.4167 - val_object_sparse_categorical_accuracy: 0.2500 - val_object_Top_3: 0.7292 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.8750\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.4292 - action_loss: 1.7612 - object_loss: 2.4172 - location_loss: 1.2509 - action_sparse_categorical_accuracy: 0.2350 - action_Top_3: 0.5750 - object_sparse_categorical_accuracy: 0.2000 - object_Top_3: 0.6250 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8200 - val_loss: 5.3398 - val_action_loss: 1.7973 - val_object_loss: 2.3365 - val_location_loss: 1.2060 - val_action_sparse_categorical_accuracy: 0.2292 - val_action_Top_3: 0.4167 - val_object_sparse_categorical_accuracy: 0.3750 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.8750\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.4085 - action_loss: 1.7701 - object_loss: 2.4157 - location_loss: 1.2227 - action_sparse_categorical_accuracy: 0.2550 - action_Top_3: 0.5600 - object_sparse_categorical_accuracy: 0.2850 - object_Top_3: 0.6400 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8150 - val_loss: 5.2121 - val_action_loss: 1.7906 - val_object_loss: 2.2636 - val_location_loss: 1.1579 - val_action_sparse_categorical_accuracy: 0.3125 - val_action_Top_3: 0.4167 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.8750\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.2921 - action_loss: 1.7628 - object_loss: 2.3445 - location_loss: 1.1847 - action_sparse_categorical_accuracy: 0.2850 - action_Top_3: 0.5900 - object_sparse_categorical_accuracy: 0.3200 - object_Top_3: 0.7000 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8150 - val_loss: 5.1324 - val_action_loss: 1.7819 - val_object_loss: 2.2190 - val_location_loss: 1.1315 - val_action_sparse_categorical_accuracy: 0.3542 - val_action_Top_3: 0.4167 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.8750\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.2160 - action_loss: 1.7525 - object_loss: 2.3103 - location_loss: 1.1532 - action_sparse_categorical_accuracy: 0.3000 - action_Top_3: 0.5950 - object_sparse_categorical_accuracy: 0.3200 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8150 - val_loss: 5.0537 - val_action_loss: 1.7716 - val_object_loss: 2.1730 - val_location_loss: 1.1091 - val_action_sparse_categorical_accuracy: 0.3542 - val_action_Top_3: 0.4375 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.8750\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.1292 - action_loss: 1.7378 - object_loss: 2.2639 - location_loss: 1.1275 - action_sparse_categorical_accuracy: 0.3050 - action_Top_3: 0.6300 - object_sparse_categorical_accuracy: 0.3250 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8250 - val_loss: 4.9144 - val_action_loss: 1.7503 - val_object_loss: 2.0843 - val_location_loss: 1.0798 - val_action_sparse_categorical_accuracy: 0.3750 - val_action_Top_3: 0.5625 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.8750\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.0387 - action_loss: 1.7245 - object_loss: 2.2213 - location_loss: 1.0929 - action_sparse_categorical_accuracy: 0.3150 - action_Top_3: 0.6550 - object_sparse_categorical_accuracy: 0.3150 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8650 - val_loss: 4.7925 - val_action_loss: 1.7234 - val_object_loss: 2.0034 - val_location_loss: 1.0658 - val_action_sparse_categorical_accuracy: 0.3750 - val_action_Top_3: 0.6458 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.9583\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.9421 - action_loss: 1.7020 - object_loss: 2.1545 - location_loss: 1.0856 - action_sparse_categorical_accuracy: 0.3500 - action_Top_3: 0.6900 - object_sparse_categorical_accuracy: 0.3200 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8800 - val_loss: 4.6694 - val_action_loss: 1.6851 - val_object_loss: 1.9248 - val_location_loss: 1.0595 - val_action_sparse_categorical_accuracy: 0.3750 - val_action_Top_3: 0.6458 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.9583\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.8601 - action_loss: 1.6600 - object_loss: 2.0847 - location_loss: 1.1153 - action_sparse_categorical_accuracy: 0.3500 - action_Top_3: 0.7150 - object_sparse_categorical_accuracy: 0.3150 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.8650 - val_loss: 4.5068 - val_action_loss: 1.6156 - val_object_loss: 1.8322 - val_location_loss: 1.0590 - val_action_sparse_categorical_accuracy: 0.3542 - val_action_Top_3: 0.7708 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.9792\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.7082 - action_loss: 1.6169 - object_loss: 1.9947 - location_loss: 1.0966 - action_sparse_categorical_accuracy: 0.4000 - action_Top_3: 0.7650 - object_sparse_categorical_accuracy: 0.3150 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.9250 - val_loss: 4.3480 - val_action_loss: 1.5557 - val_object_loss: 1.7660 - val_location_loss: 1.0264 - val_action_sparse_categorical_accuracy: 0.4167 - val_action_Top_3: 0.8125 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.9583\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.5761 - action_loss: 1.5738 - object_loss: 1.9584 - location_loss: 1.0439 - action_sparse_categorical_accuracy: 0.4300 - action_Top_3: 0.7700 - object_sparse_categorical_accuracy: 0.3150 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.9250 - val_loss: 4.1743 - val_action_loss: 1.5026 - val_object_loss: 1.7033 - val_location_loss: 0.9685 - val_action_sparse_categorical_accuracy: 0.4583 - val_action_Top_3: 0.8542 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 1.0000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 3s 3s/step - loss: 4.4119 - action_loss: 1.5188 - object_loss: 1.9224 - location_loss: 0.9708 - action_sparse_categorical_accuracy: 0.4600 - action_Top_3: 0.8600 - object_sparse_categorical_accuracy: 0.3150 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.9450 - val_loss: 3.9724 - val_action_loss: 1.4417 - val_object_loss: 1.6261 - val_location_loss: 0.9046 - val_action_sparse_categorical_accuracy: 0.5000 - val_action_Top_3: 0.8958 - val_object_sparse_categorical_accuracy: 0.3125 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.9583\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.2444 - action_loss: 1.4920 - object_loss: 1.8405 - location_loss: 0.9119 - action_sparse_categorical_accuracy: 0.4750 - action_Top_3: 0.8850 - object_sparse_categorical_accuracy: 0.3450 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6100 - location_Top_3: 0.9500 - val_loss: 3.7800 - val_action_loss: 1.3809 - val_object_loss: 1.5430 - val_location_loss: 0.8561 - val_action_sparse_categorical_accuracy: 0.5000 - val_action_Top_3: 0.9375 - val_object_sparse_categorical_accuracy: 0.3542 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6042 - val_location_Top_3: 0.9375\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.0831 - action_loss: 1.4300 - object_loss: 1.7637 - location_loss: 0.8895 - action_sparse_categorical_accuracy: 0.5450 - action_Top_3: 0.9050 - object_sparse_categorical_accuracy: 0.3950 - object_Top_3: 0.6900 - location_sparse_categorical_accuracy: 0.6150 - location_Top_3: 0.9400 - val_loss: 3.5992 - val_action_loss: 1.3208 - val_object_loss: 1.4558 - val_location_loss: 0.8226 - val_action_sparse_categorical_accuracy: 0.5833 - val_action_Top_3: 0.9792 - val_object_sparse_categorical_accuracy: 0.5417 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.6458 - val_location_Top_3: 0.8542\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.8892 - action_loss: 1.3749 - object_loss: 1.6809 - location_loss: 0.8335 - action_sparse_categorical_accuracy: 0.5150 - action_Top_3: 0.9150 - object_sparse_categorical_accuracy: 0.4700 - object_Top_3: 0.6950 - location_sparse_categorical_accuracy: 0.6650 - location_Top_3: 0.9200 - val_loss: 3.4113 - val_action_loss: 1.2583 - val_object_loss: 1.3596 - val_location_loss: 0.7935 - val_action_sparse_categorical_accuracy: 0.6667 - val_action_Top_3: 0.9792 - val_object_sparse_categorical_accuracy: 0.6250 - val_object_Top_3: 0.7708 - val_location_sparse_categorical_accuracy: 0.7083 - val_location_Top_3: 0.8542\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6783 - action_loss: 1.3183 - object_loss: 1.5952 - location_loss: 0.7649 - action_sparse_categorical_accuracy: 0.5850 - action_Top_3: 0.9700 - object_sparse_categorical_accuracy: 0.5400 - object_Top_3: 0.7000 - location_sparse_categorical_accuracy: 0.7450 - location_Top_3: 0.9400 - val_loss: 3.2328 - val_action_loss: 1.2027 - val_object_loss: 1.2681 - val_location_loss: 0.7621 - val_action_sparse_categorical_accuracy: 0.7292 - val_action_Top_3: 0.9792 - val_object_sparse_categorical_accuracy: 0.7292 - val_object_Top_3: 0.7917 - val_location_sparse_categorical_accuracy: 0.6875 - val_location_Top_3: 0.9375\n",
            "2021-11-16 04:17:36.142738: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154414080 exceeds 10% of free system memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciOEu_80SvAY",
        "outputId": "0cd584df-56ee-4d6b-d2c6-5c0c43b349c0"
      },
      "source": [
        "!python3 saarthi_test.py --config 'external.json'  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.104.183.170:8470\n",
            "2021-11-16 04:17:54.049937: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Using 8 TPUs\n",
            "Test shape is (248, 4)\n",
            "F1 score for action--> 0.7862903225806451\n",
            "F1 score for object--> 0.6653225806451613\n",
            "F1 score for location--> 0.7661290322580645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl3C-HAeUNnS"
      },
      "source": [
        "# RUNNING IN COLAB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiqGmYeCNdgf"
      },
      "source": [
        "!pip install transformers --quiet\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import TFRobertaModel\n",
        "from keras.callbacks import CSVLogger\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "import logging, sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "logging.disable(sys.maxsize)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLvMkEFFqEOb",
        "outputId": "222730e1-e2b6-4ef6-d331-1f286c0089ad"
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy, if tpus are available tpus are used\n",
        "# Else if gpus are available gpus are used. If neither are available computation is done with CPUs\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    # Distribution strategy if tpus are available and is to be used\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print('Using {} TPUs'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "elif tf.config.list_physical_devices('GPU'):\n",
        "    # Distribution strategy in case of multiple GPUs\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print('Using {} GPUs'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print('No GPU nor TPU. Running on CPU')\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.104.183.170:8470\n",
            "Using 8 TPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKMxUe90AF_I"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 13\n",
        "EPOCHS = 20\n",
        "NUM_ACTION = 6\n",
        "NUM_OBJECT = 14\n",
        "NUM_LOCATION = 4\n",
        "FOLDER_PATH = '/content/drive/MyDrive/Saarthi/'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hovL3VEl-6M7"
      },
      "source": [
        "# TRAINING (IGNORE IF NOT NEEDED)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdtpJP9CzNgh",
        "outputId": "261b7b06-2e5a-4132-b828-5145d9d3e436"
      },
      "source": [
        "train_df = pd.read_csv(FOLDER_PATH+'train_data.csv')\n",
        "val_df = pd.read_csv(FOLDER_PATH+'valid_data.csv')\n",
        "set1 = set(train_df['transcription'])\n",
        "set2 = set(val_df['transcription'])\n",
        "print('Validation data consists of same sentences as train data if the sound path is ignored.')\n",
        "train_df.drop('path',axis='columns', inplace=True)\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "full = train_df.sample(frac=1).reset_index(drop=True)\n",
        "train_df = full.loc[:199,:]\n",
        "val_df = full.loc[200:,:]\n",
        "print(\"New train shape-\",train_df.shape,\", New val shape-\",val_df.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data consists of same sentences as train data if the sound path is ignored.\n",
            "New train shape- (200, 4) , New val shape- (48, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7DSTAF0O3md"
      },
      "source": [
        "action_enc = LabelEncoder()\n",
        "action_train = action_enc.fit_transform(train_df['action'])\n",
        "action_val = action_enc.transform(val_df['action'])\n",
        "\n",
        "object_enc = LabelEncoder()\n",
        "object_train = object_enc.fit_transform(train_df['object'])\n",
        "object_val = object_enc.transform(val_df['object'])\n",
        "\n",
        "location_enc = LabelEncoder()\n",
        "location_train = location_enc.fit_transform(train_df['location'])\n",
        "location_val = location_enc.transform(val_df['location'])\n",
        "\n",
        "texts = train_df['transcription'].values\n",
        "texts = list(texts)\n",
        "val_texts = val_df['transcription'].values\n",
        "val_texts = list(val_texts)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "train_data = tokenizer(texts, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='tf')\n",
        "val_data = tokenizer(val_texts, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='tf')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y_UJfG5PEm1"
      },
      "source": [
        "y_train = {'action': action_train, 'object':object_train, 'location': location_train}\n",
        "y_val = {'action': action_val, 'object':object_val, 'location': location_val}\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_data), y_train)).batch(BATCH_SIZE)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_data), y_val)).batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swfDznpzPEkV",
        "outputId": "18f0b622-ff82-4919-f0c6-d13358eefd7d"
      },
      "source": [
        "%%time\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
        "    bert_model = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
        "    x = bert_model(ids,attention_mask=att)\n",
        "    x1 = tf.keras.layers.Flatten()(x[1])\n",
        "    x1 = tf.keras.layers.Dense(NUM_ACTION, name='action')(x1)\n",
        "\n",
        "    x2 = tf.keras.layers.Flatten()(x[1])\n",
        "    x2 = tf.keras.layers.Dense(NUM_OBJECT, name='object')(x2)\n",
        "\n",
        "    x3 = tf.keras.layers.Flatten()(x[1])\n",
        "    x3 = tf.keras.layers.Dense(NUM_LOCATION, name='location')(x3)\n",
        "    model = tf.keras.models.Model(inputs=[ids, att], outputs=[x1,x2,x3])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='Top_3')],\n",
        "        )\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "    model = build_model()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13 s, sys: 6.37 s, total: 19.4 s\n",
            "Wall time: 43.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew9-sHFmhWAp",
        "outputId": "ff7aebbf-3520-4165-dd73-23c0ccadb56e"
      },
      "source": [
        "csv_logger = CSVLogger(FOLDER_PATH+'log.csv', append=True, separator=';')\n",
        "history=model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, verbose=1, callbacks=[csv_logger])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 2s 364ms/step - loss: 0.9599 - action_loss: 0.3188 - object_loss: 0.4977 - location_loss: 0.1434 - action_sparse_categorical_accuracy: 0.9750 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9000 - object_Top_3: 0.9850 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.5830 - val_action_loss: 0.2422 - val_object_loss: 0.2784 - val_location_loss: 0.0624 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9375 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 2s 360ms/step - loss: 0.7444 - action_loss: 0.2406 - object_loss: 0.3970 - location_loss: 0.1069 - action_sparse_categorical_accuracy: 0.9800 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9500 - object_Top_3: 0.9900 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.4565 - val_action_loss: 0.1749 - val_object_loss: 0.2282 - val_location_loss: 0.0534 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9583 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 5s 893ms/step - loss: 0.5968 - action_loss: 0.1830 - object_loss: 0.3210 - location_loss: 0.0928 - action_sparse_categorical_accuracy: 0.9850 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9600 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.3683 - val_action_loss: 0.1359 - val_object_loss: 0.1912 - val_location_loss: 0.0411 - val_action_sparse_categorical_accuracy: 0.9375 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9583 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 2s 359ms/step - loss: 0.4937 - action_loss: 0.1430 - object_loss: 0.2764 - location_loss: 0.0742 - action_sparse_categorical_accuracy: 0.9900 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9600 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.2952 - val_action_loss: 0.1223 - val_object_loss: 0.1457 - val_location_loss: 0.0272 - val_action_sparse_categorical_accuracy: 0.9792 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 2s 359ms/step - loss: 0.3946 - action_loss: 0.1104 - object_loss: 0.2313 - location_loss: 0.0529 - action_sparse_categorical_accuracy: 0.9950 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9800 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.2580 - val_action_loss: 0.1160 - val_object_loss: 0.1177 - val_location_loss: 0.0244 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 2s 352ms/step - loss: 0.3122 - action_loss: 0.0871 - object_loss: 0.1769 - location_loss: 0.0482 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9950 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1952 - val_action_loss: 0.0870 - val_object_loss: 0.0881 - val_location_loss: 0.0201 - val_action_sparse_categorical_accuracy: 0.9583 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 2s 367ms/step - loss: 0.2634 - action_loss: 0.0730 - object_loss: 0.1480 - location_loss: 0.0423 - action_sparse_categorical_accuracy: 0.9950 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 0.9900 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1737 - val_action_loss: 0.0756 - val_object_loss: 0.0776 - val_location_loss: 0.0206 - val_action_sparse_categorical_accuracy: 0.9792 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 2s 358ms/step - loss: 0.2186 - action_loss: 0.0589 - object_loss: 0.1221 - location_loss: 0.0376 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1528 - val_action_loss: 0.0722 - val_object_loss: 0.0640 - val_location_loss: 0.0167 - val_action_sparse_categorical_accuracy: 0.9792 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 0.9792 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 2s 355ms/step - loss: 0.1767 - action_loss: 0.0516 - object_loss: 0.0925 - location_loss: 0.0326 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.1137 - val_action_loss: 0.0516 - val_object_loss: 0.0484 - val_location_loss: 0.0137 - val_action_sparse_categorical_accuracy: 1.0000 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 1.0000 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 2s 352ms/step - loss: 0.1557 - action_loss: 0.0440 - object_loss: 0.0845 - location_loss: 0.0272 - action_sparse_categorical_accuracy: 1.0000 - action_Top_3: 1.0000 - object_sparse_categorical_accuracy: 1.0000 - object_Top_3: 1.0000 - location_sparse_categorical_accuracy: 1.0000 - location_Top_3: 1.0000 - val_loss: 0.0962 - val_action_loss: 0.0475 - val_object_loss: 0.0372 - val_location_loss: 0.0115 - val_action_sparse_categorical_accuracy: 1.0000 - val_action_Top_3: 1.0000 - val_object_sparse_categorical_accuracy: 1.0000 - val_object_Top_3: 1.0000 - val_location_sparse_categorical_accuracy: 1.0000 - val_location_Top_3: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M0qkxdr8hmN"
      },
      "source": [
        "model.save_weights(FOLDER_PATH+'model.h5')\n",
        "np.save(FOLDER_PATH+'action_encoder.npy', action_enc.classes_)\n",
        "np.save(FOLDER_PATH+'object_encoder.npy', object_enc.classes_)\n",
        "np.save(FOLDER_PATH+'location_encoder.npy', location_enc.classes_)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpTBK1Mu66H"
      },
      "source": [
        "# PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AIpdhk9ahwA",
        "outputId": "d1c7dc5b-1149-4854-cf27-a8207269fd81"
      },
      "source": [
        "#folder containing model and the encoders\n",
        "FOLDER_PATH = '/content/drive/MyDrive/Saarthi/'\n",
        "#path to test dataset\n",
        "TEST_PATH = FOLDER_PATH+'valid_data.csv'\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "test_df.drop('path',axis='columns', inplace=True)\n",
        "test_df.drop_duplicates(inplace=True)\n",
        "print(\"Test shape is\", test_df.shape)\n",
        "\n",
        "action_enc = LabelEncoder()\n",
        "action_enc.classes_ = np.load(FOLDER_PATH+'action_encoder.npy', allow_pickle=True)\n",
        "action_test = action_enc.transform(test_df['action'])\n",
        "\n",
        "object_enc = LabelEncoder()\n",
        "object_enc.classes_ = np.load(FOLDER_PATH+'object_encoder.npy', allow_pickle=True)\n",
        "object_test = object_enc.transform(test_df['object'])\n",
        "\n",
        "location_enc = LabelEncoder()\n",
        "location_enc.classes_ = np.load(FOLDER_PATH+'location_encoder.npy', allow_pickle=True)\n",
        "location_test = location_enc.transform(test_df['location'])\n",
        "\n",
        "test_texts = test_df['transcription'].values\n",
        "test_texts = list(test_texts)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "MAX_LEN = 13\n",
        "test_data = tokenizer(test_texts, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='tf')\n",
        "y_test = {'action': action_test, 'object':object_test, 'location': location_test}\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_data), y_test)).batch(BATCH_SIZE)\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
        "    bert_model = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
        "    x = bert_model(ids,attention_mask=att)\n",
        "    x1 = tf.keras.layers.Flatten()(x[1])\n",
        "    x1 = tf.keras.layers.Dense(NUM_ACTION, name='action')(x1)\n",
        "\n",
        "    x2 = tf.keras.layers.Flatten()(x[1])\n",
        "    x2 = tf.keras.layers.Dense(NUM_OBJECT, name='object')(x2)\n",
        "\n",
        "    x3 = tf.keras.layers.Flatten()(x[1])\n",
        "    x3 = tf.keras.layers.Dense(NUM_LOCATION, name='location')(x3)\n",
        "    model = tf.keras.models.Model(inputs=[ids, att], outputs=[x1,x2,x3])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='Top_3')],\n",
        "        )\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "    model = build_model()\n",
        "\n",
        "model.load_weights(FOLDER_PATH+'model.h5')\n",
        "ans = model.predict(test_dataset)\n",
        "\n",
        "action_preds = action_enc.inverse_transform(np.argmax(ans[0],axis=1))\n",
        "object_preds = object_enc.inverse_transform(np.argmax(ans[1],axis=1))\n",
        "location_preds = location_enc.inverse_transform(np.argmax(ans[2],axis=1))\n",
        "\n",
        "pred_df = pd.DataFrame({'input':test_df['transcription'],'action':test_df['action'],'object':test_df['object'],'location':test_df['location'],\n",
        "                        'action_preds':action_preds, 'object_preds':object_preds, 'location_preds':location_preds})\n",
        "\n",
        "# micro f1 score\n",
        "action_f1 = f1_score(pred_df['action_preds'],pred_df['action'], average='micro')\n",
        "object_f1 = f1_score(pred_df['object_preds'],pred_df['object'], average='micro')\n",
        "location_f1 = f1_score(pred_df['location_preds'],pred_df['location'], average='micro')\n",
        "print('F1 score for action-->',action_f1)\n",
        "print('F1 score for object-->',object_f1)\n",
        "print('F1 score for location-->',location_f1)\n",
        "pred_df.to_csv(FOLDER_PATH+'predictions.csv',index=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test shape is (248, 4)\n",
            "F1 score for action--> 1.0\n",
            "F1 score for object--> 1.0\n",
            "F1 score for location--> 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvC88KRREKro",
        "outputId": "dbb0ddd8-b8da-4fca-a4fa-42944abf70ce"
      },
      "source": [
        "print(pred_df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          input  ... location_preds\n",
            "0                            Turn on the lights  ...           none\n",
            "1                           Turn off the lights  ...           none\n",
            "2                               Change language  ...           none\n",
            "3                               Pause the music  ...           none\n",
            "4                                        Resume  ...           none\n",
            "..                                          ...  ...            ...\n",
            "243                Turn the washroom lights off  ...       washroom\n",
            "244                                       Pause  ...           none\n",
            "326                  Lights off in the washroom  ...       washroom\n",
            "418                           Bedroom heat down  ...        bedroom\n",
            "478  OK now switch the main language to Chinese  ...           none\n",
            "\n",
            "[248 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOZLD1pwYGfd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}